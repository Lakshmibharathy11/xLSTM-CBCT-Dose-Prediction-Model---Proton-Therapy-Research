**Efficient CBCT → CT Synthesis Using Lightweight Residual UNet**

**Project Summary**


This project implements an efficient deep learning pipeline for CBCT-to-CT image translation using the SynthRAD2025 dataset.

The primary objective was to achieve competitive reconstruction quality (PSNR/SSIM) while significantly reducing model complexity compared to large 3D nnResU-Net baselines.

The result is a lightweight, production-aware model architecture optimized for:

Lower computational cost

Faster training and experimentation

Reduced memory footprint

Clean, reproducible ML pipeline design

**Problem Statement**

In radiation therapy workflows, CBCT scans are commonly used for patient positioning but contain artifacts that limit their direct clinical usability. Deep learning-based CBCT → CT translation enables improved image quality for dose calculation and verification.

State-of-the-art approaches (e.g., 3D nnResU-Net) achieve strong performance but require:

30–57M parameters

~20GB GPU memory

Heavy 3D patch-based training

**This project explores:**

Can we achieve strong reconstruction performance using a lightweight architecture designed for efficiency and scalability?

**Architecture Design**
MobileResUNetLite

A lightweight UNet-style encoder–decoder with:

Residual learning head (predicts correction rather than full mapping)

Depthwise separable convolutions (reduces FLOPs and parameter count)

Batch normalization for stable training

Mixed precision (AMP) support

Modular PyTorch implementation

**Design Principles**

Minimize parameter count while preserving feature extraction capacity

Improve gradient flow using residual prediction

Enable training on limited hardware

Keep architecture deployable for real-world inference

Training Pipeline

Framework: PyTorch

Loss Function: L1 Loss (MAE-based optimization)

Optimizer: AdamW

Mixed Precision Training (torch.cuda.amp)

Learning rate scheduling

Automatic checkpointing

Metric tracking per epoch (MAE, PSNR, SSIM)

Config-driven reproducible setup

**Dataset & Preprocessing**

Dataset: SynthRAD2025 (CBCT → CT Task)

Hounsfield Unit clipping

Z-score normalization

Paired slice-based supervised training

Train/validation split

The pipeline is structured to allow easy extension to:

3D patch-based training

Feature-based perceptual losses (AFP)

Cross-modality extensions (MR → CT)

**Validation Results**

Best Model (Epoch 18)

MAE (HU): 40.7

PSNR: 28.92 dB

SSIM: 0.9140

These results demonstrate strong structural and intensity reconstruction using a significantly lighter architecture than large 3D nnResU-Net models.

**Comparison to Published Baseline**

Reference: Deep Learning-Based Cross-Anatomy CT Synthesis using Adapted nnResU-Net (arXiv:2509.22394)

Baseline:

PSNR ≈ 31.69 dB

30–57M parameters

3D architecture

~20GB VRAM footprint

**This project:**

Uses lightweight 2D architecture

Depthwise separable convolution blocks

Reduced memory and compute requirements

Competitive PSNR relative to model size

The trade-off prioritizes efficiency and deployability while maintaining strong reconstruction performance.

Engineering Highlights

Clean modular architecture

Production-style training loop

Metric-based checkpoint selection

Mixed precision acceleration

Easily extendable to multi-loss training

Structured for experimentation and benchmarking

Scalability & Deployment Considerations

This architecture is suitable for:

Resource-constrained training environments

Faster iteration cycles

Edge or lower-cost inference scenarios

Model compression and optimization research

Future enhancements may include:

FLOPs benchmarking

Model pruning or quantization

ONNX export

Real-time inference benchmarking

AFP loss integration for anatomical feature prioritization

Skills Demonstrated

Deep Learning (CNNs, Residual Learning)

Model Efficiency Optimization

PyTorch Engineering

Medical Imaging Data Handling

Metric-driven Model Evaluation

Training Pipeline Design

Applied Research Implementation

**Author**
Lakshmi Bharathy Kumar
MS Data Science Candidate (Graduating May 2026)
**Focus Areas: Efficient Deep Learning, Applied AI, Medical Imaging, ML Systems**
